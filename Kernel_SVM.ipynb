{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate using *scikit-learn* to perform supervised clustering\n",
    "## With some advanced ML concepts\n",
    "This was originally coded in 2016, as part of a stand-alone python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahowe42/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ahowe42/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarSubset(p):\n",
    "\t\"\"\"\n",
    "\tGenerate an array of binary indices that can be used for all-subset combinatorial analysis\n",
    "\tof a dataset with p variables.\n",
    "\t---\n",
    "\tUsage: subset_binaries, subset_sizes = VarSubset(p)\n",
    "\t---\n",
    "\tp: integer indicating number of variables to subset\n",
    "\tsubset_binaries: (2^p, p) array of all subsets binary indices that can be used to subset\n",
    "\t\tinto the presumed original data matrix\n",
    "\tsubset_sizes: 2^p array indicating number of variables in each subset\n",
    "\t---\n",
    "\tex: p = 4; cols = np.arange(p); bins,sizs = QB.VarSubset(p); print(cols[bins[8,:]])\n",
    "\tJAH 20121018\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# check that p is int; could just duck-type it, but if user passes something else, something is screwed up\n",
    "\tif type(p) is not int:\n",
    "\t\traise ValueError(\"The number variables must be integer: %s\"%VarSubset.__doc__)\n",
    "\t\n",
    "\t# prepare the output array; we want bool, but have to start with int, so the assignment below works correctly\n",
    "\tsubbins = np.zeros((2**p,p),dtype=int)\n",
    "\t\n",
    "\t# loop through all subsets :-( getting the binary representations\n",
    "\tfor cnt in range(1,2**p):\n",
    "\t\t# get binary representation into a list, then put it in the array\n",
    "\t\ttmp = bin(cnt)[2:]\n",
    "\t\tsubbins[cnt,(-len(tmp)):] = list(tmp)\n",
    "\t\n",
    "\t# fill in the variable counts\n",
    "\tsubsize = np.sum(subbins,axis=1)\n",
    "\t\n",
    "\t# finally sort by variable counts\n",
    "\ttmp = np.argsort(subsize)\n",
    "\t\n",
    "\treturn subbins[tmp,:]==1, subsize[tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations = 150, Features = 4\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "(n,p) = data.shape\n",
    "print('Observations = %d, Features = %d'%(n,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 96.00%\n"
     ]
    }
   ],
   "source": [
    "''' Evaluate performance in original data space with Logistic Regression '''\n",
    "LR = LogisticRegression()\n",
    "class_rate = LR.fit(X = data, y = labels).score(X = data, y = labels)\n",
    "print('Logistic Regression: %0.2f%%'%(100*class_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel: 99.33%\n",
      "Quadratic Kernel: 98.67%\n",
      "RBF Kernel: 98.67%\n",
      "Sigmoid Kernel: 4.00%\n"
     ]
    }
   ],
   "source": [
    "''' Now evaluate performance in Feature Space with 4 kernels '''\n",
    "kerns = ['linear','quadratic','rbf','sigmoid']\n",
    "kerncnt = len(kerns)\n",
    "\n",
    "# homogenous linear SVC\n",
    "line = svm.SVC(kernel='linear')\n",
    "class_rate = line.fit(X = data, y = labels).score(X = data, y = labels)\n",
    "print('Linear Kernel: %0.2f%%'%(100*class_rate))\n",
    "# homogenous quadratic SVC\n",
    "quad = svm.SVC(kernel='poly',degree=2,gamma=1/p,coef0=0)\n",
    "class_rate = quad.fit(X = data, y = labels).score(X = data, y = labels)\n",
    "print('Quadratic Kernel: %0.2f%%'%(100*class_rate))\n",
    "# RBF SVC\n",
    "rbfn = svm.SVC(kernel='rbf',gamma=1/p)\n",
    "class_rate = rbfn.fit(X = data, y = labels).score(X = data, y = labels)\n",
    "print('RBF Kernel: %0.2f%%'%(100*class_rate))\n",
    "# sigmoid SVC\n",
    "sigm = svm.SVC(kernel='sigmoid',gamma=1/p,coef0=0)\n",
    "class_rate = sigm.fit(X = data, y = labels).score(X = data, y = labels)\n",
    "print('Sigmoid Kernel: %0.2f%%'%(100*class_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linear</th>\n",
       "      <th>quadratic</th>\n",
       "      <th>rbf</th>\n",
       "      <th>sigmoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.975167</td>\n",
       "      <td>0.961167</td>\n",
       "      <td>0.968167</td>\n",
       "      <td>0.264667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016833</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>0.020664</td>\n",
       "      <td>0.066847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           linear   quadratic         rbf     sigmoid\n",
       "count  100.000000  100.000000  100.000000  100.000000\n",
       "mean     0.975167    0.961167    0.968167    0.264667\n",
       "std      0.016833    0.020386    0.020664    0.066847\n",
       "min      0.933333    0.900000    0.916667    0.000000\n",
       "25%      0.966667    0.950000    0.950000    0.250000\n",
       "50%      0.983333    0.966667    0.966667    0.283333\n",
       "75%      0.983333    0.983333    0.983333    0.300000\n",
       "max      1.000000    1.000000    1.000000    0.316667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Overfitting to the observed data is an important concern, so use cross validation '''\n",
    "# set up the randomized cross-validator\n",
    "CVs = 100\n",
    "cvrand = cross_validation.ShuffleSplit(n, n_iter = CVs, train_size = 0.6,\\\n",
    "    test_size = 0.4, random_state = 12272010)\n",
    "cv_kern_scores = np.zeros((CVs,kerncnt),dtype=float)\n",
    "\n",
    "# homogenous linear SVC\n",
    "cv_kern_scores[:,0] = cross_validation.cross_val_score(line, data, labels,\\\n",
    "    cv=cvrand)\n",
    "# homogenous quadratic SVC\n",
    "cv_kern_scores[:,1] = cross_validation.cross_val_score(quad, data, labels,\\\n",
    "    cv=cvrand)\n",
    "# RBF SVC\n",
    "cv_kern_scores[:,2] = cross_validation.cross_val_score(rbfn, data, labels,\\\n",
    "    cv=cvrand)\n",
    "# sigmoid SVC\n",
    "cv_kern_scores[:,3] = cross_validation.cross_val_score(sigm, data, labels,\\\n",
    "    cv=cvrand)\n",
    "\n",
    "# summarize\n",
    "scores = pd.DataFrame(data = cv_kern_scores,columns = kerns)\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Score = 97.52%\n",
      "\tcoef0 = -1\n",
      "\tdegree = 1\n",
      "\tgamma = 1\n",
      "\tkernel = 'poly'\n"
     ]
    }
   ],
   "source": [
    "''' Instead of arbitrarily setting the parameters, let's do a grid search among reasonable alternatives '''\n",
    "# seach the subspace of possible parameters\n",
    "params = [{'kernel':['poly'],'degree':[1,2,3],'gamma':[1/p,1,2],'coef0':[-1,0,1]},\\\n",
    "\t{'kernel':['rbf'],'gamma':[1/p,1,2],'degree':[3],'coef0':[0]},\\\n",
    "\t{'kernel':['sigmoid'],'gamma':[1/p,1,2],'coef0':[-1,0,1],'degree':[3]}]\n",
    "GSC = grid_search.GridSearchCV(estimator = svm.SVC(), param_grid = params,\\\n",
    "    cv = cvrand, n_jobs = -1)\n",
    "GSC.fit(X = data, y = labels)\n",
    "\n",
    "# print the results\n",
    "print('Best Model Score = %0.2f%%'%(100*GSC.best_score_))\n",
    "for param in GSC.best_params_.keys():\n",
    "\tprint('\\t%s = %r'%(param,GSC.best_params_[param]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       0.975167\n",
       "std        0.016833\n",
       "min        0.933333\n",
       "25%        0.966667\n",
       "50%        0.983333\n",
       "75%        0.983333\n",
       "max        1.000000\n",
       "Name: poly, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now run the best model using cross-validation & summarize it's performance\n",
    "best_mod = svm.SVC(kernel=GSC.best_params_['kernel'],\\\n",
    "    gamma=GSC.best_params_['gamma'],coef0=GSC.best_params_['coef0'],\\\n",
    "    degree=GSC.best_params_['degree'])\n",
    "best_scores = cross_validation.cross_val_score(best_mod, data, labels,\\\n",
    "    cv=cvrand)\n",
    "pd.Series(data = best_scores, name = GSC.best_params_['kernel']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset array([False, False, False,  True]) (0)\n",
      "Subset array([False, False,  True, False]) (1)\n",
      "Subset array([False,  True, False, False]) (2)\n",
      "Subset array([ True, False, False, False]) (3)\n",
      "Subset array([False, False,  True,  True]) (4)\n",
      "Subset array([False,  True, False,  True]) (5)\n",
      "Subset array([False,  True,  True, False]) (6)\n",
      "Subset array([ True, False, False,  True]) (7)\n",
      "Subset array([ True, False,  True, False]) (8)\n",
      "Subset array([ True,  True, False, False]) (9)\n",
      "Subset array([False,  True,  True,  True]) (10)\n",
      "Subset array([ True, False,  True,  True]) (11)\n",
      "Subset array([ True,  True, False,  True]) (12)\n",
      "Subset array([ True,  True,  True, False]) (13)\n",
      "Subset array([ True,  True,  True,  True]) (14)\n",
      "Best Subset of Features: array([ True,  True,  True,  True])\n",
      "Best Kernel SVM Model: {'coef0': -1, 'degree': 1, 'gamma': 1, 'kernel': 'poly'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       0.975167\n",
       "std        0.016833\n",
       "min        0.933333\n",
       "25%        0.966667\n",
       "50%        0.983333\n",
       "75%        0.983333\n",
       "max        1.000000\n",
       "Name: poly, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' To be able to select the best subset of features, run everything seen on all subsets '''\n",
    "# generate all subsets ...\n",
    "bins = VarSubset(p)[0][1:]\n",
    "subset_cnt = len(bins)\n",
    "best_mods = []\n",
    "best_scs = [0]*subset_cnt\n",
    "# ... and run the GSC\n",
    "for ind,sub in enumerate(bins):\n",
    "    print('Subset %r (%d)'%(sub,ind))\n",
    "    # cross-validated grid search\n",
    "    GSC.fit(X = data[:,sub], y = labels)\n",
    "    # save the best model & score per subset\n",
    "    best_mods.append(GSC.best_params_)\n",
    "    best_scs[ind] = GSC.best_score_\n",
    "# now find the subset with the best score and run the model\n",
    "bst = np.argmax(best_scs)\n",
    "best_model = best_mods[bst]\n",
    "best_subset = bins[bst,:]\n",
    "print('Best Subset of Features: %r\\nBest Kernel SVM Model: %r'%\\\n",
    "    (best_subset,best_model))\n",
    "# now run the best model using cross-validation & summarize it's performance\n",
    "best_mod = svm.SVC(kernel=best_model['kernel'],\\\n",
    "    gamma=best_model['gamma'],coef0=best_model['coef0'],\\\n",
    "    degree=best_model['degree'])\n",
    "best_scores = cross_validation.cross_val_score(best_mod, data[:,best_subset],\\\n",
    "    labels,cv=cvrand)\n",
    "pd.Series(data = best_scores, name = GSC.best_params_['kernel']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
